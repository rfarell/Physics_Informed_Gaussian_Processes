{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of Symplectic Spectrum Gaussian Processes\n",
    "\n",
    "This notebook is an exploration and implementation based on the work of Yusuke Tanaka, Tomoharu Iwata, and Naonori Ueda from NTT Communication Science Laboratories. The original code from the authors can be found at [SSGP on GitHub](https://github.com/yusuk-e/SSGP).\n",
    "\n",
    "## Problem Statement\n",
    "The primary problem the authors are addressing is learning the dynamics of Hamiltonian systems from noisy and sparse data. In mathematical terms, they are focusing on systems where the evolution over time can be described by Hamiltonian mechanics. The Hamiltonian, $ H(\\boldsymbol{x}) $, represents the total energy of the system, and is a function of the state $ \\boldsymbol{x} $ in phase space. This state $ \\boldsymbol{x} $ combines both generalized coordinates $ \\boldsymbol{x}^{\\mathrm{q}} $ and momenta $ \\boldsymbol{x}^{\\mathrm{p}} $.\n",
    "\n",
    "The key equation governing the dynamics of such a system is given by:\n",
    "\n",
    "$$\n",
    "\\frac{d \\boldsymbol{x}}{d t} = (\\mathbf{S} - \\mathbf{R}) \\nabla H(\\boldsymbol{x}) =: \\boldsymbol{f}(\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{S} $ is a skew-symmetric matrix, and $ \\mathbf{R} $ is a positive semi-definite dissipation matrix, which introduces dissipative effects like friction into the system. When $ \\mathbf{R} = \\mathbf{O} $ (the zero matrix), the system conserves total energy. The function $ \\boldsymbol{f}(\\boldsymbol{x}) $ represents the time derivatives of the state and has a symplectic geometric structure.\n",
    "\n",
    "The authors' goal is to model these dynamics using a probabilistic model, specifically a Gaussian Process (GP) that accounts for the symplectic structure of Hamiltonian systems. This model is aimed at effectively learning from data that is both sparse (limited in quantity and low in temporal resolution) and noisy. They propose the Symplectic Spectrum Gaussian Processes (SSGPs), which utilizes random Fourier features to efficiently approximate the Hamiltonian dynamics and allows for variational inference using ordinary differential equation solvers. \n",
    "\n",
    "The innovation lies in the ability to predict the dynamics of such systems from arbitrary initial conditions and to decompose the dynamics into conservative and dissipative terms, which is particularly challenging when the available data is sparse and noisy. The approach extends to scenarios where derivative observations are unavailable, relying instead on trajectory data.\n",
    "\n",
    "\n",
    "## Paper Example\n",
    "![](https://cdn.mathpix.com/cropped/2024_01_06_22f307116735454ec646g-10.jpg?height=598&width=1372&top_left_y=250&top_left_x=366)\n",
    "\n",
    "Prediction results of SSGP. The color indicates time-evolution, starting at blue and ending at red. The first and second columns are the true trajectories for the dissipative systems and their conservative terms, respectively. The third column is the prediction for the dissipative systems in Task 1. The fourth and fifth columns are the predicted conservative and dissipative terms in Task 2, respectively. Here, the dissipative terms are multiplied by 30 for enhanced clarity. Comparisons with other models are shown in Appendix $\\mathrm{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Systems\n",
    "\n",
    "The authors test the Symplectic Spectrum Gaussian Processes (SSGP) model on two physical systems: the pendulum and Duffing oscillator. The Hamiltonians are defined as follows:\n",
    "\n",
    "Pendulum System:\n",
    "$$\n",
    "H(\\boldsymbol{x}) = 2 m g l(1 - \\cos x^{\\mathrm{q}}) + \\frac{l^{2}(x^{\\mathrm{p}})^{2}}{2 m}\n",
    "$$\n",
    "Parameters: $ g = 3 $, $ m = l = 1 $.\n",
    "\n",
    "Duffing Oscillator System:\n",
    "$$\n",
    "H(\\boldsymbol{x}) = \\frac{1}{2}(x^{\\mathrm{p}})^{2} + \\frac{\\alpha}{2}(x^{\\mathrm{q}})^{2} + \\frac{\\beta}{4}(x^{\\mathrm{q}})^{4}\n",
    "$$\n",
    "\n",
    "### Tasks\n",
    "\n",
    "**Task 1: Normal Prediction**\n",
    "Performance was assessed by comparing predicted state trajectories $\\{\\hat{\\boldsymbol{x}}_{i j}\\}$ against ground truth $\\{\\boldsymbol{x}_{i j}^{\\text{true}}\\}$ from the test set. The metric was mean squared error (MSE): $\\frac{1}{I} \\sum_{i=1}^{I}\\left(\\frac{1}{J_{i}} \\sum_{j=1}^{J_{i}}\\|\\hat{\\boldsymbol{x}}_{i j}-\\boldsymbol{x}_{i j}^{\\text{true}}\\|^{2}\\right)$. Evaluations were done for pendulum and Duffing oscillator systems, both with and without energy dissipation.\n",
    "\n",
    "**Task 2: Predicting Dynamics for Unseen Friction Coefficients**\n",
    "We showcased SSGP's ability to separate conservative and dissipative dynamics by training on datasets with friction coefficients ($ \\mathbf{R} = \\operatorname{diag}(0,0.05) $), then predicting conservative system dynamics ($ \\mathbf{R} = \\mathbf{O} $). The same MSE metric was used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Overview\n",
    "\n",
    "![](https://cdn.mathpix.com/cropped/2024_01_06_22f307116735454ec646g-04.jpg?height=333&width=1374&top_left_y=251&top_left_x=381)\n",
    "\n",
    "Schematic diagram of SSGP: Generative processes of noisy trajectories. (a) We model the unknown Hamiltonian $H(\\boldsymbol{x})$ by a single-output GP. Here, the color represents the magnitude of energy. (b) The vector field $\\boldsymbol{f}(\\boldsymbol{x})$ is calculated by applying the differential operator $\\mathcal{L}$ to $H(\\boldsymbol{x})$. (c) We sample the initial condition from the standard Gaussian distribution and solve the ODE defined by $\\boldsymbol{f}(\\boldsymbol{x})$ to obtain the noiseless trajectory $\\left\\{\\boldsymbol{x}_{i j}\\right\\}$ depicted by black dots. (d) The noisy trajectory $\\left\\{\\boldsymbol{y}_{i j}\\right\\}$ depicted by red dots is observed by adding Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Mechanics\n",
    "\n",
    "Consider a system with $N$ degrees of freedom. In the Hamiltonian formalism, the continuous-time evolution of the system is described in phase space, that is, the product space of generalized coordinates $\\boldsymbol{x}^{\\mathrm{q}}=\\left(x_{1}^{\\mathrm{q}}, \\ldots, x_{N}^{\\mathrm{q}}\\right)$ and generalized momenta $\\boldsymbol{x}^{\\mathrm{p}}=\\left(x_{1}^{\\mathrm{p}}, \\ldots, x_{N}^{\\mathrm{p}}\\right)$. Let $\\boldsymbol{x}=\\left(\\boldsymbol{x}^{\\mathrm{q}}, \\boldsymbol{x}^{\\mathrm{p}}\\right) \\in \\mathbb{R}^{D}$ be a state of the system, where $D=2 N$. The system's evolution is determined by the Hamiltonian $H(\\boldsymbol{x}): \\mathbb{R}^{D} \\rightarrow \\mathbb{R}$, which denotes the system's total energy. Traditionally, the Hamiltonian is manually designed to suit the system. The dynamics of a Hamiltonian system with additive dissipative terms is given by\n",
    "\n",
    "$$\n",
    "\\frac{d \\boldsymbol{x}}{d t}=(\\mathbf{S}-\\mathbf{R}) \\nabla H(\\boldsymbol{x})=: \\boldsymbol{f}(\\boldsymbol{x}), \\quad \\text { where } \\quad \\mathbf{S}=\\left(\\begin{array}{cc}\n",
    "\\mathbf{O} & \\mathbf{I} \\\\\n",
    "-\\mathbf{I} & \\mathbf{O} \n",
    "\\end{array}\\right) \\quad \\quad (1)\n",
    "$$\n",
    "\n",
    "Here, $\\nabla H(\\boldsymbol{x}): \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$ is the gradient of the Hamiltonian with respect to state $\\boldsymbol{x}, \\mathbf{S} \\in \\mathbb{R}^{D \\times D}$ is the skew-symmetric matrix, $\\mathbf{R} \\in \\mathbb{R}^{D \\times D}$ is the positive semi-definite dissipation matrix, $\\mathbf{I}$ is the identity matrix, and $\\mathbf{O}$ is the zero matrix. In (1), we define the time derivatives of the state by the function $\\boldsymbol{f}(\\boldsymbol{x}): \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$, which is a special kind of vector field that has a symplectic geometric structure (called Hamiltonian vector field or symplectic gradient). The dynamics on this vector field conserve the total energy when $\\mathbf{R}=\\mathbf{O}$. Given vector field $\\boldsymbol{f}(x)$ and initial condition $\\boldsymbol{x}_{1}$ at time $t_{1}$, one can predict state $\\boldsymbol{x}_{t}$ at time $t$ by integrating $\\boldsymbol{f}(\\boldsymbol{x})$ from $t_{1}$ to $t$, as follows: $\\boldsymbol{x}_{t}=\\boldsymbol{x}_{1}+\\int_{t_{1}}^{t} \\boldsymbol{f}(\\boldsymbol{x}) d t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP Priors for Hamiltonian Systems with Additive Dissipation\n",
    "\n",
    "In SSGP, the Hamiltonian $ H(\\boldsymbol{x}) $ is modeled as a Gaussian Process (GP) with zero mean. The vector field $ \\boldsymbol{f}(\\boldsymbol{x}) $ is derived from this GP using the differential operator $ \\mathcal{L} = (\\mathbf{S} - \\mathbf{R}) \\nabla $:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}(\\boldsymbol{x}) = \\mathcal{L} H(\\boldsymbol{x}), \\quad H(\\boldsymbol{x}) \\sim \\mathcal{G}\\mathcal{P}\\left(0, \\gamma(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime})\\right)  \\quad \\quad (2)\n",
    "$$\n",
    "\n",
    "Here, $ \\gamma(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) $ is the covariance function. The vector field is a multi-output GP:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}(\\boldsymbol{x}) \\sim \\mathcal{G}\\mathcal{P}(\\mathbf{0}, \\mathbf{K}(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}))  \\quad \\quad (3)\n",
    "$$\n",
    "\n",
    "The covariance function $ \\mathbf{K}(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) $ incorporates the geometric structure of Hamiltonian systems:\n",
    "\n",
    "$$\n",
    "\\mathbf{K}(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\mathcal{L} \\mathcal{L}^{\\top} \\gamma(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = (\\mathbf{S} - \\mathbf{R}) \\nabla^{2}(\\mathbf{S} - \\mathbf{R})^{\\top} \\gamma(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime})  \\quad \\quad (4)\n",
    "$$\n",
    "\n",
    "A common choice for $ \\gamma $ is the ARD Gaussian kernel:\n",
    "\n",
    "$$\n",
    "\\gamma(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}) = \\sigma_{0}^{2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}^{\\prime})^{\\top} \\boldsymbol{\\Lambda}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}^{\\prime})\\right)  \\quad \\quad (5)\n",
    "$$\n",
    "\n",
    "where $ \\sigma_{0}^{2} $ is the signal variance and $ \\boldsymbol{\\Lambda} = \\operatorname{diag}(\\lambda_{1}^{2}, \\ldots, \\lambda_{D}^{2}) $ are the length scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Representations\n",
    "\n",
    "In SSGP, the Hamiltonian $H(\\boldsymbol{x})$ is approximated as a Gaussian Process (GP) using Random Fourier Features (RFF) that capture symplectic structures in Hamiltonian systems. This approach is particularly beneficial for estimating GP posteriors and efficiently sampling vector fields, especially when only trajectory data are available.\n",
    "\n",
    "The Hamiltonian is represented as:\n",
    "\n",
    "$$\n",
    "H(\\boldsymbol{x}) = \\sum_{m=1}^{M} \\boldsymbol{w}_{m} \\boldsymbol{\\phi}_{m}(\\boldsymbol{x}), \\quad \\boldsymbol{w}_{m} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\frac{\\sigma_{0}^{2}}{M} \\mathbf{I}\\right)  \\quad \\quad (6)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\phi}_{m}(\\boldsymbol{x})$ are the basis functions and $\\boldsymbol{w}_{m}$ are the weights. The spectral points $s_{m}$ are sampled from the spectral density of the kernel:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{s}) = \\mathcal{N}\\left(\\mathbf{0},\\left(4 \\pi^{2} \\boldsymbol{\\Lambda}\\right)^{-1}\\right)  \\quad \\quad (7)\n",
    "$$\n",
    "\n",
    "Applying the differential operator $\\mathcal{L}$ to this approximation yields the spectral representation of the vector field:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}(\\boldsymbol{x}) = \\mathcal{L} H(\\boldsymbol{x}) =: \\boldsymbol{\\Psi}(\\boldsymbol{x}) \\boldsymbol{w}^{\\top}  \\quad \\quad (8)\n",
    "$$\n",
    "\n",
    "with the feature maps $\\boldsymbol{\\Psi}(\\boldsymbol{x})$ defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Psi}_{m}(\\boldsymbol{x}) = 2 \\pi(\\mathbf{S}-\\mathbf{R}) \\boldsymbol{s}_{m}\\left[-\\sin \\left(2 \\pi \\boldsymbol{s}_{m}^{\\top} \\boldsymbol{x}\\right), \\cos \\left(2 \\pi \\boldsymbol{s}_{m}^{\\top} \\boldsymbol{x}\\right)\\right]  \\quad \\quad (9)\n",
    "$$\n",
    "\n",
    "These are referred to as Symplectic Random Fourier Features (S-RFF). This novel integration of symplectic structures into random features bridges GP modeling and Hamiltonian mechanics. The distribution of $\\boldsymbol{f}$ is obtained by integrating out $\\boldsymbol{w}$:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{f}) = \\mathcal{N}\\left(\\mathbf{0}, \\tilde{\\mathbf{K}}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)\\right), \\quad \\tilde{\\mathbf{K}}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right) = \\frac{\\sigma_{0}^{2}}{M} \\boldsymbol{\\Psi}(\\boldsymbol{x}) \\boldsymbol{\\Psi}\\left(\\boldsymbol{x}^{\\prime}\\right)^{\\top}  \\quad \\quad (10)\n",
    "$$\n",
    "\n",
    "The covariance function $\\tilde{\\mathbf{K}}$ for the GP approximation improves in quality with more spectral points, as shown by comparing the gram matrices of $\\mathbf{K}$ and $\\tilde{\\mathbf{K}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative processes of noisy observations\n",
    "Suppose that we have a collection of $I$ trajectories $\\left\\{\\left(t_{i j}, \\boldsymbol{y}_{i j}\\right) \\mid i=1, \\ldots, I ; j=1, \\ldots, J_{i}\\right\\}$, where $J_{i}$ is the number of samples in the $i$ th trajectory. Each sample is specified by a pair $\\left(t_{i j}, \\boldsymbol{y}_{i j}\\right)$, which represents the observation of noisy state $\\boldsymbol{y}_{i j}$ at time $t_{i j}$. We treat the noiseless state $\\boldsymbol{x}_{i j}$, the counterpart of $\\boldsymbol{y}_{i j}$, as a latent variable. We assume that the observation model of $\\boldsymbol{y}_{i j}$ is a Gaussian distribution with a variance of $\\sigma^{2}$. Letting $\\mathbf{Y}=\\left\\{\\boldsymbol{y}_{i j}\\right\\}$, the marginal likelihood (i.e., evidence) is given by\n",
    "\n",
    "$$\n",
    "p(\\mathbf{Y})=\\int p(\\boldsymbol{f}) \\prod_{i=1}^{I}\\left[\\int p\\left(\\boldsymbol{y}_{i 1} \\mid \\boldsymbol{x}_{i 1}\\right) p\\left(\\boldsymbol{x}_{i 1}\\right) \\prod_{j=2}^{J_{i}} p\\left(\\boldsymbol{y}_{i j} \\mid \\boldsymbol{x}_{i j}\\right) p\\left(\\boldsymbol{x}_{i j} \\mid \\boldsymbol{f}, \\boldsymbol{x}_{i 1}\\right) d \\boldsymbol{x}_{i 1}\\right] d \\boldsymbol{f}  \\quad \\quad (11)\n",
    "$$\n",
    "\n",
    "where $p(\\boldsymbol{f})$ is the GP prior (10) of the vector field, and $p\\left(\\boldsymbol{x}_{i 1}\\right)$ is the prior distribution ${ }^{4}$ of the initial condition $\\boldsymbol{x}_{i 1}$. Given $\\boldsymbol{f}$ and $\\boldsymbol{x}_{i 1}$, the state $\\boldsymbol{x}_{i j}$ is deterministically given by solving the ODE; thus, we can write the conditional distribution $p\\left(\\boldsymbol{x}_{i j} \\mid \\boldsymbol{f}, \\boldsymbol{x}_{i 1}\\right)$ in (11) using Dirac's delta function, as follows:\n",
    "\n",
    "$$\n",
    "p\\left(\\boldsymbol{x}_{i j} \\mid \\boldsymbol{f}, \\boldsymbol{x}_{i 1}\\right)=\\delta\\left(\\boldsymbol{x}_{i j}-\\left[\\boldsymbol{x}_{i 1}+\\int_{t_{i 1}}^{t_{i j}} \\boldsymbol{f}(\\boldsymbol{x}) d t\\right]\\right)  \\quad \\quad (12)\n",
    "$$\n",
    "\n",
    "Note that, although we omit the observation time points $\\left\\{t_{i j}\\right\\}$ in (11), it is actually conditioned on $\\left\\{t_{i j}\\right\\}$. For simplicity, we adopt this notation hereinafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Exact calculation of the marginal likelihood in our model is intractable due to the ODE solving process. \n",
    "\n",
    "### Parameter Learning\n",
    "We use the evidence lower bound (ELBO) for parameter estimation:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{Y}) \\geq \\sum_{i=1}^{I}\\left[\\sum_{j=1}^{J_{i}} \\mathbb{E}_{q\\left(\\boldsymbol{x}_{i j}\\right)}\\left[\\log p\\left(\\boldsymbol{y}_{i j} \\mid \\boldsymbol{x}_{i j}\\right)\\right]-\\mathrm{KL}\\left[q\\left(\\boldsymbol{x}_{i 1}\\right) \\| p\\left(\\boldsymbol{x}_{i 1}\\right)\\right]\\right]-\\mathrm{KL}[q(\\boldsymbol{w}) \\| p(\\boldsymbol{w})] \\quad (13)\n",
    "$$\n",
    "\n",
    "The variational distributions for the state $\\boldsymbol{x}_{i j}$ and weights $\\boldsymbol{w}$ are assumed to be Gaussian. For the initial condition, the variational distribution is Gaussian centered at the observed state:\n",
    "\n",
    "$$\n",
    "q\\left(\\boldsymbol{x}_{i j}\\right) = \\iint p\\left(\\boldsymbol{x}_{i j} \\mid \\boldsymbol{f}, \\boldsymbol{x}_{i 1}\\right)\\left[\\int p(\\boldsymbol{f} \\mid \\boldsymbol{w}) q(\\boldsymbol{w}) d \\boldsymbol{w}\\right] q\\left(\\boldsymbol{x}_{i 1}\\right) d \\boldsymbol{x}_{i 1} d \\boldsymbol{f} \\quad (14)\n",
    "$$\n",
    "\n",
    "The expectation in ELBO is approximated using Monte Carlo integration:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q\\left(\\boldsymbol{x}_{i j}\\right)}\\left[\\log p\\left(\\boldsymbol{y}_{i j} \\mid \\boldsymbol{x}_{i j}\\right)\\right] \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\log p\\left(\\boldsymbol{y}_{i j} \\mid \\boldsymbol{x}_{i j}^{(k)}\\right) \\quad (15)\n",
    "$$\n",
    "\n",
    "Monte Carlo samples are generated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{i 1}^{(k)} & = \\boldsymbol{y}_{i 1}+\\sqrt{\\mathbf{A}} \\boldsymbol{\\epsilon}_{i}^{(k)}, \\quad \\boldsymbol{\\epsilon}_{i}^{(k)} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\quad (16) \\\\\n",
    "\\boldsymbol{x}_{i 2}^{(k)}, \\ldots, \\boldsymbol{x}_{i J_{i}}^{(k)} & = \\text{ODESolve}\\left(\\boldsymbol{x}_{i 1}^{(k)}, \\boldsymbol{f}^{(k)}(\\boldsymbol{x}), t_{i 2}, \\ldots, t_{i J_{i}}\\right) \\quad (17)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The sample of the vector field from the variational posterior is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}^{(k)}(\\boldsymbol{x})=\\boldsymbol{\\Psi}(\\boldsymbol{x})\\left[\\frac{1}{L} \\sum_{l=1}^{L} \\boldsymbol{w}^{(k, l)}\\right]^{\\top}, \\quad \\boldsymbol{w}^{(k, l)}=\\boldsymbol{b}+\\sqrt{\\mathbf{C}} \\boldsymbol{\\epsilon}^{(k, l)} \\quad (18)\n",
    "$$\n",
    "\n",
    "Parameter optimization accounts for uncertainties in the vector field by sampling $\\boldsymbol{w}$ during training. Our model extends to high-dimensional data by combining with an autoencoder, where $\\boldsymbol{x}$ and $\\boldsymbol{x}^{\\prime}$ in $\\mathbf{K}(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime})$ are latent vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "The variational posterior $q(\\boldsymbol{f})$ is Gaussian with mean $\\tilde{\\boldsymbol{m}}^{*}(\\boldsymbol{x})=\\boldsymbol{\\Psi}(\\boldsymbol{x}) \\boldsymbol{b}^{\\top}$ and covariance $\\tilde{\\mathbf{K}}^{*}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)=\\boldsymbol{\\Psi}(\\boldsymbol{x}) \\mathbf{C} \\boldsymbol{\\Psi}^{\\top}\\left(\\boldsymbol{x}^{\\prime}\\right)$. Predictions and their uncertainties are made by integrating $\\tilde{\\boldsymbol{m}}^{*}(\\boldsymbol{x})$ and using $\\tilde{\\mathbf{K}}^{*}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)$.\n",
    "\n",
    "### On Computational Complexity\n",
    "The primary computational cost lies in sampling $w$, specifically in computing $\\sqrt{\\mathbf{C}}$ which is $\\mathcal{O}\\left(M^{3}\\right)$. While cost increases with more basis functions, it is important that this cost is not part of the ODE solver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from autograd) (1.26.0)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from autograd) (0.18.3)\n",
      "Requirement already satisfied: torchdiffeq in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torchdiffeq) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torchdiffeq) (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from scipy>=1.4.0->torchdiffeq) (1.26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install autograd\n",
    "! pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def csv_read(file):\n",
    "    f = open(file)\n",
    "    csvReader = csv.reader(f)\n",
    "    D = []\n",
    "    for row in csvReader:\n",
    "        D.append(row)\n",
    "    return D\n",
    "\n",
    "def csv_write(file, D):\n",
    "    f = open(file,'w')\n",
    "    csvWriter = csv.writer(f,lineterminator='\\n')\n",
    "    if np.ndim(D) == 1:\n",
    "        csvWriter.writerow(D)\n",
    "    elif np.ndim(D) == 2:\n",
    "        for i in range(np.shape(D)[0]):\n",
    "            line = D[i]\n",
    "            csvWriter.writerow(line)\n",
    "    f.close()\n",
    "\n",
    "def pkl_read(file):\n",
    "    f = open(file, 'rb')\n",
    "    D = pickle.load(f)\n",
    "    f.close()\n",
    "    return D\n",
    "\n",
    "def pkl_write(file, D):\n",
    "    f = open(file,'wb')\n",
    "    pickle.dump(D,f,protocol=4)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "import pdb\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import autograd.numpy as np\n",
    "import autograd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import os, sys\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "%pwd\n",
    "THIS_DIR = %pwd  # This will set THIS_DIR to the current working directory\n",
    "\n",
    "xmin = -3.2; xmax = 3.2; ymin = -3.2; ymax = 3.2\n",
    "DPI = 200\n",
    "FORMAT = 'pdf'\n",
    "LINE_SEGMENTS = 10\n",
    "ARROW_SCALE = 100\n",
    "ARROW_WIDTH = 6e-3\n",
    "LINE_WIDTH = 2\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Args({\n",
    "    'batch_time': 1,        \n",
    "    'learn_rate': 1e-3,      \n",
    "    'total_steps': 100,     \n",
    "    'print_every': 100,      \n",
    "    'sigma': 0.1,            \n",
    "    'eta': 0.0,                \n",
    "    'samples': 10,           \n",
    "    'timescale': 3,          \n",
    "    'name': 'pendulum',      \n",
    "    's': 0,                  \n",
    "    'gridsize': 15,          \n",
    "    'seed': 0,              \n",
    "    'num_basis': 100,      \n",
    "    'friction': False,      \n",
    "    'train_samples': 5, \n",
    "    'val_samples': 5,\n",
    "    'datasets': 5,\n",
    "    'T': 5,\n",
    "    'radius_a': 1.,\n",
    "    'radius_b': 1.,\n",
    "    'seed': 0,\n",
    "    'save_dir': THIS_DIR,\n",
    "    'input_dim': 2\n",
    "})\n",
    "\n",
    "\n",
    "class ODE_pendulum(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.M = self.permutation_tensor(input_dim)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        H = self.H(x)\n",
    "        dH = torch.autograd.grad(H.sum(), x)[0]\n",
    "        field = dH @ self.M.t()\n",
    "        dH[:,0] = 0\n",
    "        field = field - args.eta * dH\n",
    "        return field\n",
    "    \n",
    "    def time_derivative(self, x):\n",
    "        H = self.H(x)\n",
    "        dH = torch.autograd.grad(H.sum(), x)[0]\n",
    "        field = dH @ self.M.t()\n",
    "        dH[:,0] = 0\n",
    "        field = field - args.eta * dH\n",
    "        return field\n",
    "        \n",
    "    def H(self, coords):\n",
    "        q, p = coords[:,0], coords[:,1]\n",
    "        H = 3*(1-torch.cos(q)) + p**2\n",
    "        return H\n",
    "\n",
    "    def permutation_tensor(self,n):\n",
    "        M = torch.eye(n)\n",
    "        M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "        return M\n",
    "\n",
    "class ODE_duffing(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.M = self.permutation_tensor(input_dim)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        H = self.H(x)\n",
    "        dH = torch.autograd.grad(H.sum(), x)[0]\n",
    "        field = dH @ self.M.t()\n",
    "        dH[:,0] = 0\n",
    "        field = field - args.eta * dH\n",
    "        return field\n",
    "    \n",
    "    def time_derivative(self, x):\n",
    "        H = self.H(x)\n",
    "        dH = torch.autograd.grad(H.sum(), x)[0]\n",
    "        field = dH @ self.M.t()\n",
    "        dH[:,0] = 0\n",
    "        field = field - args.eta * dH\n",
    "        return field\n",
    "        \n",
    "    def H(self, coords):\n",
    "        if len(coords) == 2:\n",
    "            q, p = coords[0], coords[1]\n",
    "        else:\n",
    "            q, p = coords[:,0], coords[:,1]\n",
    "        H = .5*p**2 - .5*q**2 + .25*q**4\n",
    "        return H\n",
    "\n",
    "    def permutation_tensor(self,n):\n",
    "        M = torch.eye(n)\n",
    "        M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "        return M\n",
    "\n",
    "def vis_obs(save_dir, field, data, trajectory_name):\n",
    "    y = data[trajectory_name]\n",
    "    t_eval = data['t']\n",
    "    fig = plt.figure(figsize=(11.3, 21), facecolor='white', dpi=DPI)\n",
    "    N = y.shape[0]\n",
    "    N = 28 if N > 28 else N\n",
    "    for i in range(N):\n",
    "        t = torch.tensor(np.linspace(0, t_eval[-1], t_eval.shape[0]))\n",
    "        ax = fig.add_subplot(math.ceil(N/4), 4, i+1, frameon=True)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.quiver(field['x'][:,0], field['x'][:,1], field['dx'][:,0], field['dx'][:,1],\n",
    "                   scale=ARROW_SCALE, width=ARROW_WIDTH,\n",
    "                   cmap='gray_r', color=(.5,.5,.5))\n",
    "        ax.scatter(y[i][:,0], y[i][:,1], c=t, s=16, cmap='coolwarm')\n",
    "        plt.axis([xmin, xmax, ymin, ymax])\n",
    "        plt.xlabel(\"$q$\", fontsize=12)\n",
    "        plt.ylabel(\"$p$\", rotation=0, fontsize=12)\n",
    "        plt.title(\"Sample \" + str(i+1))\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('{}/{}.{}'.format(save_dir, trajectory_name, FORMAT))\n",
    "    plt.close()\n",
    "\n",
    "def vis_energy(save_dir, data, e_name):\n",
    "    es = data[e_name]\n",
    "    t_eval = data['t']\n",
    "    \n",
    "    fig = plt.figure(figsize=(11.3, 21), facecolor='white', dpi=DPI)\n",
    "    N = es.shape[0]\n",
    "    N = 28 if N > 28 else N\n",
    "    for i in range(N):\n",
    "        t = t_eval\n",
    "        ax = fig.add_subplot(math.ceil(N/4), 4, i+1, frameon=True)\n",
    "        ax.plot(t, es[i],'-', color='black')\n",
    "        ymax = data['es'].max()\n",
    "        ax.axis([0, args.T, 0, ymax*1.5])\n",
    "        plt.xlabel(\"time\", fontsize=12)\n",
    "        plt.ylabel(\"Energy\", rotation=90, fontsize=12)\n",
    "        plt.title(\"Sample \" + str(i+1))\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('{}/{}.{}'.format(save_dir, e_name, FORMAT))\n",
    "    plt.close()\n",
    "\n",
    "def get_field(ode, xmin, xmax, ymin, ymax, gridsize):\n",
    "    field = {}\n",
    "\n",
    "    # meshgrid to get vector field\n",
    "    b, a = np.meshgrid(np.linspace(xmin, xmax, gridsize), np.linspace(ymin, ymax, gridsize))\n",
    "    x = np.stack([b.flatten(), a.flatten()]).T\n",
    "    x = torch.tensor(x, requires_grad=True)\n",
    "\n",
    "    # get vector directions\n",
    "    dx = ode.time_derivative(x)\n",
    "    field['x'] = x.detach().numpy()\n",
    "    field['dx'] = dx.detach().numpy()\n",
    "    field['mesh_a'] = a\n",
    "    field['mesh_b'] = b\n",
    "    return field\n",
    "\n",
    "def vis_field(save_dir, field, name):\n",
    "    a = field['mesh_a']\n",
    "    b = field['mesh_b']\n",
    "    fig = plt.figure(figsize=(4,3), facecolor='white', dpi=DPI)\n",
    "    ax = fig.subplots()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    scale = ARROW_SCALE\n",
    "    ax.quiver(field['x'][:,0], field['x'][:,1], field['dx'][:,0], field['dx'][:,1],\n",
    "              scale=scale, width=ARROW_WIDTH)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('{}/{}.{}'.format(save_dir, name, FORMAT))\n",
    "    plt.close()\n",
    "\n",
    "def get_init():\n",
    "\n",
    "    N = samples\n",
    "    if args.name in ['pendulum']:\n",
    "        np.random.seed(args.seed)\n",
    "        x0s = np.random.rand(N,2)*2.-1\n",
    "        x0s = x0s.T\n",
    "        np.random.seed(args.seed)\n",
    "        radius = np.random.rand(N,1)*args.radius_a+args.radius_b\n",
    "        x0s = (x0s / np.sqrt((x0s**2).sum(0))).T * radius\n",
    "        x0s = torch.tensor(x0s, requires_grad=True)\n",
    "\n",
    "    elif args.name in ['duffing']:\n",
    "        x0s = []\n",
    "        np.random.seed(args.seed)\n",
    "        while(1):\n",
    "            x0 = np.random.rand(2)*6.-3\n",
    "            en = ode.H(torch.tensor(x0))\n",
    "            if (args.radius_b <= en) and (en <= args.radius_a+args.radius_b):\n",
    "                x0s.append(x0)\n",
    "                if len(x0s) == N:\n",
    "                    break\n",
    "        x0s = torch.tensor(np.stack(x0s), requires_grad=True)\n",
    "\n",
    "    return x0s\n",
    "\n",
    "def path_arrange(path):\n",
    "  x = []\n",
    "  for i in range(path.shape[1]):\n",
    "    x.append(path[:,i,:])\n",
    "  return torch.stack(x)\n",
    "\n",
    "def generate_data(ode):\n",
    "\n",
    "    data = {'meta': locals()}\n",
    "    dt = 1/args.timescale\n",
    "    \n",
    "    xs, ys, dys, es = [], [], [], []\n",
    "    x0s = get_init()\n",
    "    t = torch.tensor(np.linspace(0, args.T, int(args.timescale*args.T+1)))\n",
    "    xs = odeint(ode, x0s, t, method='dopri5', atol=1e-8, rtol=1e-8)\n",
    "    xs = path_arrange(xs)\n",
    "    for x in xs:\n",
    "        e = ode.H(x)\n",
    "        es.append(e)\n",
    "    es = torch.stack(es)\n",
    "    np.random.seed(args.seed)\n",
    "    noise = np.random.normal(0,args.sigma,[xs.shape[0],xs.shape[1],xs.shape[2]])\n",
    "    ys = xs + torch.tensor(noise)\n",
    "    \n",
    "    for y in ys:\n",
    "        dy = torch.diff(y, dim=0) / dt\n",
    "        dys.append(dy)\n",
    "    dys = torch.stack(dys)\n",
    "\n",
    "    data['xs'] = xs.detach().numpy()\n",
    "    data['ys'] = ys.detach().numpy()\n",
    "    data['dys'] = dys.detach().numpy()\n",
    "    data['t'] = t.detach().numpy()\n",
    "    data['es'] = es.detach().numpy()\n",
    "    field = get_field(ode, xmin, xmax, ymin, ymax, args.gridsize)\n",
    "\n",
    "    vis_obs(save_dir, field, data, trajectory_name='xs')\n",
    "    vis_obs(save_dir, field, data, trajectory_name='ys')\n",
    "    vis_field(save_dir, field, 'field')\n",
    "    vis_energy(save_dir, data, e_name='es')\n",
    "    return data\n",
    "\n",
    "def split(s):\n",
    "    train_split_id = args.train_samples\n",
    "    ids = [i for i in range(samples)]\n",
    "    if samples == 10:\n",
    "        np.random.seed(s)\n",
    "        np.random.shuffle(ids)\n",
    "        train_ids = ids[:train_split_id]; val_ids = ids[train_split_id:]\n",
    "    else:\n",
    "        if samples == 15:\n",
    "            prev_samples = 10\n",
    "        elif samples == 20:\n",
    "            prev_samples = 15\n",
    "        elif samples == 30:\n",
    "            prev_samples = 20\n",
    "        elif samples == 50:\n",
    "            prev_samples = 30\n",
    "            \n",
    "        prev_dir = ( args.save_dir + '/' + args.name  + '/' + str(args.eta) + '/train/'\n",
    "                    + str(args.sigma) + '/' + str(prev_samples) + '/' + str(args.timescale))\n",
    "        filename = prev_dir + '/' + str(s) + '/train_ids.csv'\n",
    "        train_ids = csv_read(filename)\n",
    "        train_ids = list(np.array(train_ids[0]).astype(int))\n",
    "\n",
    "        a_ids = tuple(set(ids) - set(train_ids))\n",
    "        a_ids = [a_ids[i] for i in range(len(a_ids))]\n",
    "        a_samples = train_split_id - len(train_ids)\n",
    "\n",
    "        np.random.seed(s)\n",
    "        np.random.shuffle(a_ids)\n",
    "        train_ids.extend(a_ids[:a_samples]); val_ids = a_ids[a_samples:]\n",
    "\n",
    "    split_data = {}\n",
    "    for k in ['xs', 'ys', 'dys', 'es']:\n",
    "        split_data['val_' + k], split_data[k] = data[k][val_ids], data[k][train_ids]\n",
    "    split_data['val_t'], split_data['t'] = data['t'], data['t']\n",
    "\n",
    "    return split_data, train_ids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samples = args.train_samples + args.val_samples\n",
    "    if args.name == 'pendulum':\n",
    "        ode = ODE_pendulum(args.input_dim)\n",
    "    elif args.name == 'duffing':\n",
    "        ode = ODE_duffing(args.input_dim)\n",
    "\n",
    "    save_dir = ( args.save_dir + '/' + args.name  + '/' + str(args.eta) + '/train/' + str(args.sigma)\n",
    "                 + '/' + str(samples) + '/' + str(args.timescale))\n",
    "    os.makedirs(save_dir) if not os.path.exists(save_dir) else None\n",
    "    data = generate_data(ode)\n",
    "    pkl_write(save_dir + '/data.pkl', data)\n",
    "    field = get_field(ode, xmin, xmax, ymin, ymax, args.gridsize)\n",
    "    pkl_write(save_dir + '/field.pkl', field)\n",
    "\n",
    "    for s in range(args.datasets):\n",
    "        split_data, train_ids = split(s)\n",
    "        save_dir_s = save_dir + '/' + str(s)\n",
    "        os.makedirs(save_dir_s) if not os.path.exists(save_dir_s) else None\n",
    "        pkl_write(save_dir_s + '/dataset.pkl', split_data)\n",
    "        csv_write(save_dir_s + '/train_ids.csv', train_ids)\n",
    "\n",
    "        vis_obs(save_dir_s, field, split_data, trajectory_name='ys')\n",
    "        vis_obs(save_dir_s, field, split_data, trajectory_name='val_ys')\n",
    "        \n",
    "\n",
    "    filename = '{}/{}.json'.format(save_dir, args.name)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(vars(args), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dpi=100\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "sns.set_context(\"paper\", 1.5, {\"lines.linewidth\": 1.5})\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "def plot(file, x, ys, xlabel, ylabel, legend):\n",
    "    colors = ['blue','orange','green']\n",
    "    fig = plt.figure(figsize=(8, 4), facecolor='white', dpi=dpi)\n",
    "    for i in range(2):\n",
    "        ax = fig.add_subplot(1, 2, i+1, frameon=True)\n",
    "        ax.plot(x, ys[i], c=colors[i])\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        plt.title(legend[i])\n",
    "        ax.yaxis.offsetText.set_fontsize(16)\n",
    "        plt.gca().ticklabel_format(style=\"sci\", scilimits=(0,0), axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file, format='pdf')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os, torch, pickle, zipfile\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "DPI = 200\n",
    "FORMAT = 'pdf'\n",
    "LINE_SEGMENTS = 10\n",
    "ARROW_SCALE = 100\n",
    "ARROW_WIDTH = 6e-3\n",
    "LINE_WIDTH = 2\n",
    "xmin = -3.2; xmax = 3.2; ymin = -3.2; ymax = 3.2\n",
    "\n",
    "\n",
    "def get_batch(args, x, t_eval, batch_step):\n",
    "  n_samples, n_points, input_dim = x.shape\n",
    "  N = n_samples\n",
    "  n_ids = torch.from_numpy(np.arange(N))\n",
    "  p_ids = torch.from_numpy(np.random.choice(np.arange(n_points-batch_step, dtype=np.int64), N, replace=True))\n",
    "  batch_x0 = x[n_ids,p_ids].reshape([N,1,input_dim])\n",
    "  batch_step += 1\n",
    "  batch_t = t_eval[:batch_step]\n",
    "  batch_x = ( torch.stack([x[n_ids, p_ids+i] for i in range(batch_step)], dim=0)\n",
    "              .reshape([batch_step,N,1,input_dim]) )\n",
    "  return batch_x0, batch_t, batch_x\n",
    "\n",
    "def arrange(args, x, t_eval):\n",
    "  n_samples, n_points, input_dim = x.shape\n",
    "  n_ids = np.arange(n_samples, dtype=np.int64)\n",
    "  p_ids = np.array([0]*n_samples)\n",
    "  batch_x0 = x[n_ids,p_ids].reshape([n_samples,1,input_dim])\n",
    "  batch_t = t_eval\n",
    "  batch_x = torch.stack([x[n_ids, p_ids+i] for i in range(n_points)],dim=0).reshape([n_points,n_samples,1,input_dim])\n",
    "  return batch_x0, batch_t, batch_x\n",
    "\n",
    "def get_field(func, xmin, xmax, ymin, ymax, gridsize):\n",
    "  field = {'meta': locals()}\n",
    "\n",
    "  # meshgrid to get vector field\n",
    "  b, a = np.meshgrid(np.linspace(xmin, xmax, gridsize), np.linspace(ymin, ymax, gridsize))\n",
    "  ys = np.stack([b.flatten(), a.flatten()])\n",
    "  ys = torch.tensor( ys, dtype=torch.float64, requires_grad=True).t()\n",
    "\n",
    "  # get vector directions\n",
    "  dydt = func(torch.tensor([0]),ys)\n",
    "  field['x'] = ys.cpu().detach().numpy()\n",
    "  field['dx'] = dydt.squeeze().cpu().detach().numpy()\n",
    "  field['mesh_a'] = a\n",
    "  field['mesh_b'] = b\n",
    "  return field\n",
    "\n",
    "def vis_path(filename, field, y, t, xmin, xmax, ymin, ymax):\n",
    "  fig = plt.figure(figsize=(21, 11.3), facecolor='white', dpi=DPI)\n",
    "  N = y.shape[0]\n",
    "  N = 28 if N > 28 else N\n",
    "  for i in range(N):\n",
    "    ax = fig.add_subplot(math.ceil(N/7), 7, i+1, frameon=True)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.quiver(field['x'][:,0], field['x'][:,1], field['dx'][:,0], field['dx'][:,1],\n",
    "              scale=ARROW_SCALE, width=ARROW_WIDTH,\n",
    "              cmap='gray_r', color=(.5,.5,.5))\n",
    "    ax.scatter(y[i][:,0], y[i][:,1], c=t, s=0.5, cmap='coolwarm')\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.xlabel(\"$x_q$\", fontsize=12)\n",
    "    plt.ylabel(\"$x_p$\", rotation=0, fontsize=12)\n",
    "    plt.title(\"Sample \" + str(i+1))\n",
    "    plt.grid(False)\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(filename)\n",
    "  plt.close()\n",
    "\n",
    "def vis_path_2d(filename, y, t_eval):\n",
    "  fig = plt.figure(figsize=(21, 11.3), facecolor='white', dpi=DPI)\n",
    "  N = y.shape[0]\n",
    "  N = 28 if N > 28 else N\n",
    "  for i in range(N):\n",
    "    xs1 = y[i,:,0]; xs2 = y[i,:,1]\n",
    "    ax = fig.add_subplot(math.ceil(N/7), 7, i+1, frameon=True)\n",
    "    ax.scatter(t_eval, xs1, s=0.5)\n",
    "    ax.scatter(t_eval, xs2, s=0.5)\n",
    "    plt.axis([t_eval.min().item(), t_eval.max().item(), -3, 3])\n",
    "    plt.xlabel(\"$t$\", fontsize=12)\n",
    "    plt.ylabel(\"$x_q$\", rotation=0, fontsize=12)\n",
    "    plt.title(\"Sample \" + str(i+1))\n",
    "    plt.grid(False)\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(filename)\n",
    "  plt.close()\n",
    "\n",
    "def vis_field(filename, field, xmin, xmax, ymin, ymax):\n",
    "  fig = plt.figure(figsize=(4,3), facecolor='white', dpi=DPI)\n",
    "  ax = fig.subplots()\n",
    "  ax.set_aspect('equal', adjustable='box')\n",
    "  scale = ARROW_SCALE\n",
    "  ax.quiver(field['x'][:,0], field['x'][:,1], field['dx'][:,0], field['dx'][:,1],\n",
    "            scale=scale, width=ARROW_WIDTH,\n",
    "            cmap='gray_r', color=(.5,.5,.5))\n",
    "  plt.axis([xmin, xmax, ymin, ymax])\n",
    "  plt.xlabel(\"$x_q$\", fontsize=12)\n",
    "  plt.ylabel(\"$x_p$\", rotation=0, fontsize=12)\n",
    "  plt.grid(False)\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(filename)\n",
    "  plt.close()\n",
    "\n",
    "def vis_err(filename, es, t):\n",
    "  fig = plt.figure(figsize=(21, 11.3), facecolor='white', dpi=DPI)\n",
    "  N = es.shape[0]\n",
    "  N = 28 if N > 28 else N\n",
    "  for i in range(N):\n",
    "    ax = fig.add_subplot(math.ceil(N/7), 7, i+1, frameon=True)\n",
    "    ax.plot(t, es[i],'-', color='black')\n",
    "    ax.axis([0, t.max(), 0, es.max()*1.2])\n",
    "    plt.xlabel(\"time\", fontsize=12)\n",
    "    plt.ylabel(\"MSE\", rotation=90, fontsize=12)\n",
    "    plt.title(\"Sample \" + str(i+1))\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(filename)\n",
    "  plt.close()\n",
    "\n",
    "def vis_energy(filename, true, es, t):\n",
    "  fig = plt.figure(figsize=(21, 11.3), facecolor='white', dpi=DPI)\n",
    "  N = es.shape[0]\n",
    "  N = 28 if N > 28 else N\n",
    "  if es.max() > 0:\n",
    "    ymax = es.max() if true.max() < es.max() else true.max()\n",
    "  else:\n",
    "    ymax = es.min() if true.min() > es.min() else true.min()\n",
    "  for i in range(N):\n",
    "    ax = fig.add_subplot(math.ceil(N/7), 7, i+1, frameon=True)\n",
    "    ax.plot(t, true[i],'-', color='black')\n",
    "    ax.plot(t, es[i],'-', color='red')\n",
    "    ax.axis([0, t.max(), 0, ymax*1.2])\n",
    "    plt.xlabel(\"time\", fontsize=12)\n",
    "    plt.ylabel(\"Energy\", rotation=90, fontsize=12)\n",
    "    plt.title(\"Sample \" + str(i+1))\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(filename)\n",
    "  plt.close()\n",
    "\n",
    "def path_arrange(path):\n",
    "  x = []\n",
    "  for i in range(path.shape[1]):\n",
    "    x.append(path[:,i,:])\n",
    "  return torch.stack(x)\n",
    "\n",
    "def d_pendulum_energy(coords):\n",
    "  q1, q2, p1, p2 = coords[:,:,0], coords[:,:,1], coords[:,:,2], coords[:,:,3]\n",
    "  m1 = .2; m2 = .1\n",
    "  H = ( (m2*p1**2 + (m1+m2)*p2**2 - 2*m2*p1*p2*np.cos(q1-q2))\n",
    "        / (2*m2*(m1+m2*np.sin(q1-q2)**2))\n",
    "        - (m1+m2)*9.8*np.cos(q1)\n",
    "        - m2*9.8*np.cos(q2) )\n",
    "  return H\n",
    "\n",
    "def pendulum_energy(coords):\n",
    "    qs = coords[:,:,0]; ps = coords[:,:,1]\n",
    "    energy = 3*(1-np.cos(qs)) + ps**2\n",
    "    return energy\n",
    "\n",
    "def duffing_energy(coords):\n",
    "    qs = coords[:,:,0]; ps = coords[:,:,1]\n",
    "    energy= .5*ps**2 - .5*qs**2 + .25*qs**4\n",
    "    return energy\n",
    "\n",
    "def real_pend_energy(coords):\n",
    "    qs = coords[:,:,0]; ps = coords[:,:,1]\n",
    "    energy = 2.4*(1-np.cos(qs)) + ps**2\n",
    "    return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "##This code is served in https://github.com/steveli/pytorch-sqrtm.git\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "\n",
    "class MatrixSquareRoot(Function):\n",
    "    \"\"\"Square root of a positive definite matrix.\n",
    "\n",
    "    NOTE: matrix square root is not differentiable for matrices with\n",
    "          zero eigenvalues.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        m = input.detach().cpu().numpy().astype(np.float_)\n",
    "        sqrtm = torch.from_numpy(scipy.linalg.sqrtm(m).real).to(input)\n",
    "        ctx.save_for_backward(sqrtm)\n",
    "        return sqrtm\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            sqrtm, = ctx.saved_tensors\n",
    "            sqrtm = sqrtm.data.cpu().numpy().astype(np.float_)\n",
    "            gm = grad_output.data.cpu().numpy().astype(np.float_)\n",
    "\n",
    "            # Given a positive semi-definite matrix X,\n",
    "            # since X = X^{1/2}X^{1/2}, we can compute the gradient of the\n",
    "            # matrix square root dX^{1/2} by solving the Sylvester equation:\n",
    "            # dX = (d(X^{1/2})X^{1/2} + X^{1/2}(dX^{1/2}).\n",
    "            grad_sqrtm = scipy.linalg.solve_sylvester(sqrtm, sqrtm, gm)\n",
    "\n",
    "            grad_input = torch.from_numpy(grad_sqrtm).to(grad_output)\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "sqrtm = MatrixSquareRoot.apply\n",
    "\n",
    "\n",
    "def main():\n",
    "    from torch.autograd import gradcheck\n",
    "    k = torch.randn(20, 10).double()\n",
    "    # Create a positive definite matrix\n",
    "    pd_mat = (k.t().matmul(k)).requires_grad_()\n",
    "    test = gradcheck(sqrtm, (pd_mat,))\n",
    "    print(test)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "import sys\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "class SSGP(torch.nn.Module):\n",
    "  def __init__(self, input_dim, basis, friction):\n",
    "    super(SSGP, self).__init__()\n",
    "    self.sigma = nn.Parameter(torch.tensor([1e-1]))\n",
    "    self.a = nn.Parameter(torch.ones(input_dim)*1e-1)\n",
    "    self.b = nn.Parameter(1e-4 * (torch.rand(basis*2)-0.5))\n",
    "    self.init_C(basis)\n",
    "    self.sigma_0 = nn.Parameter(torch.tensor([1e-0]))\n",
    "    self.lam = nn.Parameter(torch.ones(input_dim)*1.5)\n",
    "    if friction:\n",
    "      self.eta = nn.Parameter(torch.tensor([1e-16]))\n",
    "    else:\n",
    "      self.eta = torch.tensor([0.0])\n",
    "    self.M = self.permutation_tensor(input_dim)\n",
    "    np.random.seed(0)\n",
    "    tmp = torch.tensor(np.random.normal(0, 1, size=(int(basis/2.), input_dim)))\n",
    "    self.epsilon = torch.vstack([tmp,-tmp])\n",
    "    self.d = input_dim\n",
    "    self.num_basis = basis\n",
    "\n",
    "  def sampling_epsilon_f(self):\n",
    "    C = self.make_C()\n",
    "    sqrt_C = sqrtm(C)\n",
    "    sqrt_C = torch.block_diag(sqrt_C,sqrt_C)\n",
    "    epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T\n",
    "    self.w = self.b + (sqrt_C @ epsilon).squeeze()\n",
    "    num = 99\n",
    "    for i in range(num):\n",
    "      epsilon = torch.tensor(np.random.normal(0, 1, size=(1,sqrt_C.shape[0]))).T\n",
    "      self.w += self.b + (sqrt_C @ epsilon).squeeze()\n",
    "    self.w = self.w/(num+1)\n",
    "\n",
    "  def mean_w(self):\n",
    "    self.w = self.b * 1\n",
    "    \n",
    "  def forward(self, t, x):\n",
    "    s = self.epsilon @ torch.diag((1 / torch.sqrt(4*math.pi**2 * self.lam**2)))\n",
    "    R = torch.eye(self.d)\n",
    "    R[:int(self.d/2),:int(self.d/2)] = 0\n",
    "    mat = 2*math.pi*((self.M-self.eta**2*R)@s.T).T\n",
    "    x = x.squeeze()\n",
    "    samples = x.shape[0]\n",
    "    sim = 2*math.pi*s@x.squeeze().T\n",
    "    basis_s = -torch.sin(sim); basis_c = torch.cos(sim)\n",
    "\n",
    "    # deterministic\n",
    "    tmp = []\n",
    "    for i in range(self.d):\n",
    "      tmp.extend([mat[:,i]]*samples)\n",
    "    tmp = torch.stack(tmp).T\n",
    "    aug_mat = torch.vstack([tmp,tmp])\n",
    "    aug_s = torch.hstack([basis_s]*self.d); aug_c = torch.hstack([basis_c]*self.d)\n",
    "    aug_basis = torch.vstack([aug_s, aug_c])\n",
    "    PHI = aug_mat * aug_basis\n",
    "    aug_W = torch.stack([self.w]*samples*self.d).T\n",
    "    F = PHI * aug_W\n",
    "    f = torch.vstack(torch.split(F.sum(axis=0),samples)).T\n",
    "    return f.reshape([samples,1,self.d])\n",
    "\n",
    "  def neg_loglike(self, batch_x, pred_x):\n",
    "    n_samples, n_points, dammy, input_dim = batch_x.shape\n",
    "    likelihood = ( (-(pred_x-batch_x)**2/self.sigma**2/2).nansum()\n",
    "                   - torch.log(self.sigma**2)/2*n_samples*n_points*input_dim)\n",
    "    return -likelihood\n",
    "\n",
    "  def KL_x0(self, x0):\n",
    "    n, d = x0.shape\n",
    "    S = torch.diag(self.a**2)\n",
    "    return .5*((x0*x0).sum() + n*torch.trace(S) - n*torch.logdet(S))\n",
    "\n",
    "  def KL_w(self):\n",
    "    num = self.b.shape[0]\n",
    "    C = self.make_C()\n",
    "    C = torch.block_diag(C,C)\n",
    "    term3 = (self.b*self.b).sum() / (self.sigma_0**2 / num * 2)\n",
    "    term2 = torch.diag(C).sum() / (self.sigma_0**2 / num * 2)\n",
    "    term1_1 = torch.log(self.sigma_0**2 / num * 2) * num\n",
    "    term1_2 = torch.logdet(C)\n",
    "    return .5*( term1_1 - term1_2 + term2 + term3)\n",
    "\n",
    "  def sampling_x0(self, x0):\n",
    "    n, _, d = x0.shape\n",
    "    return (x0 + torch.sqrt(torch.stack([self.a**2]*n).reshape([n,1,d]))\n",
    "            * (torch.normal(0,1, size=(x0.shape[0],1,x0.shape[2]))))\n",
    "\n",
    "  def permutation_tensor(self,n):\n",
    "    M = torch.eye(n)\n",
    "    M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "    return M\n",
    "\n",
    "  def init_C(self, basis):\n",
    "    C = torch.linalg.cholesky(torch.ones(basis,basis)*1e-2+torch.eye(basis)*1e-2)\n",
    "    C_line = C.reshape([(basis)**2])\n",
    "    ids = torch.where(C_line!=0)[0]\n",
    "    self.c = nn.Parameter(C_line[ids])\n",
    "    ids = []\n",
    "    for i in range(basis):\n",
    "      for j in range(i+1):\n",
    "        ids.append([i,j])\n",
    "    ids = torch.tensor(ids)\n",
    "    self.ids0 = ids[:,0]\n",
    "    self.ids1 = ids[:,1]\n",
    "    \n",
    "  def make_C(self):\n",
    "    C = torch.zeros(self.num_basis,self.num_basis)\n",
    "    C[self.ids0,self.ids1] = self.c\n",
    "    C = C@C.T\n",
    "    return C\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.001\n",
      "./pendulum/0.0/train/0.1/10/3/0\n",
      "step 0, time 4.35e-02, train_loss 2.0027e+02, val_loss 2.7076e+02\n",
      "step 100, time 9.43e+00, train_loss 2.2586e+01, val_loss 8.1981e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# learning\u001b[39;00m\n\u001b[1;32m    124\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 125\u001b[0m model, stats, train_loss, val_loss, step \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m s_batch_x0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msampling_x0(batch_y0)\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mmean_w()\n\u001b[0;32m---> 72\u001b[0m pred_val_x \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_batch_x0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdopri5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m val_neg_loglike \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mneg_loglike(batch_ys, pred_val_x)\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m val_neg_loglike\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/solvers.py:30\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[0;32m---> 30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_advance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:194\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_t \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m n_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_num_steps exceeded (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps)\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adaptive_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrk_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _interp_evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39minterp_coeff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1, next_t)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:255\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    250\u001b[0m         dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Must be arranged as doing all the step_t handling, then all the jump_t handling, in case we\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# trigger both. (i.e. interleaving them would be wrong.)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y1, f1, y1_error, k \u001b[38;5;241m=\u001b[39m \u001b[43m_runge_kutta_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# dtypes:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# y1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# f1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#                     Error Ratio                      #\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[1;32m    265\u001b[0m error_ratio \u001b[38;5;241m=\u001b[39m _compute_error_ratio(y1_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol, y0, y1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:76\u001b[0m, in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m         perturb \u001b[38;5;241m=\u001b[39m Perturb\u001b[38;5;241m.\u001b[39mNONE\n\u001b[1;32m     75\u001b[0m     yi \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(k[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (beta_i \u001b[38;5;241m*\u001b[39m dt), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview_as(f0)\n\u001b[0;32m---> 76\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     k \u001b[38;5;241m=\u001b[39m _UncheckedAssign\u001b[38;5;241m.\u001b[39mapply(k, f, (\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tableau\u001b[38;5;241m.\u001b[39mbeta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mall()):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# This property (true for Dormand-Prince) lets us save a few FLOPs.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m, in \u001b[0;36mSSGP.forward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m samples \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39ms\u001b[38;5;129m@x\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 56\u001b[0m basis_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m)\u001b[49m; basis_c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(sim)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# deterministic\u001b[39;00m\n\u001b[1;32m     59\u001b[0m tmp \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Symplectic Spectrum Gaussian Processes | 2022\n",
    "# Yusuke Tanaka\n",
    "\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "#num_threads = '1'\n",
    "#os.environ['OMP_NUM_THREADS'] = num_threads\n",
    "#os.environ['MKL_NUM_THREADS'] = num_threads\n",
    "#os.environ['NUMEXPR_NUM_THREADS'] = num_threads\n",
    "\n",
    "import pdb\n",
    "import os, sys\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "%pwd\n",
    "THIS_DIR = %pwd  # This will set THIS_DIR to the current working directory\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "DPI = 200\n",
    "FORMAT = 'pdf'\n",
    "LINE_SEGMENTS = 10\n",
    "ARROW_SCALE = 100\n",
    "ARROW_WIDTH = 6e-3\n",
    "LINE_WIDTH = 2\n",
    "xmin = -3.2; xmax = 3.2; ymin = -3.2; ymax = 3.2\n",
    "\n",
    "\n",
    "# Now you can access the arguments using dot notation\n",
    "print(args.batch_time)\n",
    "print(args.learn_rate)\n",
    "\n",
    "def train():\n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # init model and optimizer\n",
    "    output_dim = input_dim\n",
    "    model = SSGP(input_dim, args.num_basis, args.friction).double()\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate)\n",
    "\n",
    "    # train loop\n",
    "    stats = {'train_loss': [], 'val_loss': []}\n",
    "    t0 = time.time()\n",
    "    min_val_loss = 1e+10\n",
    "    for step in range(args.total_steps+1):\n",
    "\n",
    "        # train step\n",
    "        batch_y0, batch_t, batch_ys = get_batch(args, ys, t_eval, batch_step)\n",
    "        s_batch_x0 = model.sampling_x0(batch_y0)\n",
    "        model.sampling_epsilon_f()\n",
    "        pred_x = odeint(model, s_batch_x0, batch_t, method='dopri5', atol=1e-8, rtol=1e-8)\n",
    "        neg_loglike = model.neg_loglike(batch_ys, pred_x)\n",
    "        KL_x0 = model.KL_x0(batch_y0.squeeze())\n",
    "        KL_w = model.KL_w()\n",
    "        loss = neg_loglike + KL_x0 + KL_w\n",
    "        loss.backward(); optim.step(); optim.zero_grad()\n",
    "        train_loss = loss.detach().item()/batch_y0.shape[0]/batch_t.shape[0]\n",
    "        # run validation data\n",
    "        with torch.no_grad():\n",
    "            batch_y0, batch_t, batch_ys = arrange(args, val_ys, t_eval)\n",
    "            s_batch_x0 = model.sampling_x0(batch_y0)\n",
    "            model.mean_w()\n",
    "            pred_val_x = odeint(model, s_batch_x0, t_eval, method='dopri5', atol=1e-8, rtol=1e-8)\n",
    "            val_neg_loglike = model.neg_loglike(batch_ys, pred_val_x)\n",
    "            loss = val_neg_loglike\n",
    "            val_loss = loss.item()/batch_y0.shape[0]/t_eval.shape[0]\n",
    "\n",
    "        # logging\n",
    "        stats['train_loss'].append(train_loss)\n",
    "        stats['val_loss'].append(val_loss)\n",
    "        if step % args.print_every == 0:\n",
    "            print(\"step {}, time {:.2e}, train_loss {:.4e}, val_loss {:.4e}\"\n",
    "                  .format(step, time.time()-t0, train_loss, val_loss))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            min_val_loss = val_loss; best_train_loss = train_loss\n",
    "            best_step = step\n",
    "            \n",
    "    return best_model, stats, best_train_loss, min_val_loss, best_step\n",
    "\n",
    "def param_save(model):\n",
    "    csv_write(save_dir + '/' + 'sigma.csv', model['sigma'].cpu().detach().numpy())\n",
    "    csv_write(save_dir + '/' + 'a.csv', model['a'].cpu().detach().numpy())\n",
    "    csv_write(save_dir + '/' + 'b.csv', model['b'].cpu().detach().numpy())\n",
    "    csv_write(save_dir + '/' + 'c.csv', model['c'].cpu().detach().numpy())\n",
    "    csv_write(save_dir + '/' + 'sigma_0.csv', model['sigma_0'].cpu().detach().numpy())\n",
    "    csv_write(save_dir + '/' + 'lam.csv', model['lam'].cpu().detach().numpy())\n",
    "    if args.friction:\n",
    "        csv_write(save_dir + '/' + 'eta.csv', model['eta'].cpu().detach().numpy())\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # setting\n",
    "    label = 'SSGP/' + str(args.num_basis)\n",
    "    i_dir = ( './' + args.name + '/' + str(args.eta) + '/train/' + str(args.sigma) \n",
    "              + '/' + str(args.samples) + '/' + str(args.timescale) + '/' + str(args.s))\n",
    "    print(i_dir)\n",
    "    save_dir = ( './' + args.name + '/' + str(args.eta) + '/result/' + str(args.sigma) \n",
    "                 + '/' + str(args.samples) + '/' + str(args.timescale) + '/' + str(args.s) + '/' + label)\n",
    "    os.makedirs(save_dir) if not os.path.exists(save_dir) else None\n",
    "\n",
    "    # input\n",
    "    filename = i_dir + '/dataset.pkl'\n",
    "    data = pkl_read(filename)\n",
    "    ys = torch.tensor( data['ys'], requires_grad=False)\n",
    "    val_ys = torch.tensor( data['val_ys'], requires_grad=False)\n",
    "    t_eval = torch.tensor( data['t'])\n",
    "    n_samples, n_points, input_dim = ys.shape\n",
    "    batch_step = int(((len(t_eval)-1)/t_eval[-1]).item() * args.batch_time)\n",
    "\n",
    "    # learning\n",
    "    t0 = time.time()\n",
    "    model, stats, train_loss, val_loss, step = train()\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # save\n",
    "    path = '{}/model.tar'.format(save_dir)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    param_save(model.state_dict())\n",
    "    \n",
    "    path = '{}/model.json'.format(save_dir)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "    path = '{}/result.csv'.format(save_dir)\n",
    "    csv_write(path, np.array(['val_step',step,'train_loss',train_loss,\n",
    "                                     'val_loss',val_loss,'train_time',train_time]))\n",
    "\n",
    "    # vis\n",
    "    ## learning curve\n",
    "    filename = save_dir + '/learning_curve.pdf'\n",
    "    x = np.arange(len(stats['train_loss']))\n",
    "    plot(filename, x, [stats['train_loss'],stats['val_loss']],\n",
    "                  'epoch','neg_loglike', ['train','validation'])\n",
    "\n",
    "    ## pred field\n",
    "    if input_dim == 2:\n",
    "        model.mean_w()\n",
    "        pred_field = get_field(model.forward, xmin, xmax, ymin, ymax, args.gridsize)\n",
    "        filename = save_dir + '/pred_field.pdf'\n",
    "        vis_field(filename, pred_field, xmin, xmax, ymin, ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
